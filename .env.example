# Set to true if you want to use Ollama for LLM inference
USE_OLLAMA=